# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.4.2
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# ### Load dataframes with text and visual attributes

# %%
import pandas as pd
import numpy as np
import os
import scipy.spatial
import scipy.stats as ss

FIGURES_DIR=os.path.join('..', 'figures')

# %%
text_df = pd.read_csv(os.path.join('..', 'data', 'text_attributes_all.csv'))
img_df = pd.read_csv(os.path.join('..', 'data', 'image_attributes_bedzju.csv')).drop_duplicates()
img_df_2 = pd.read_csv(os.path.join('..', 'data', 'image_attributes_nawrba.csv')).drop_duplicates()


# %%
img_df_not_trending = pd.read_csv(os.path.join('..', 'data', 'image_attributes_bedzju_not_trending_corr.csv')).drop_duplicates()
img_df_2_not_trending = pd.read_csv(os.path.join('..', 'data', 'image_attributes_not_trending_nawrba.csv')).drop_duplicates()
text_df_not_trending = pd.read_csv(os.path.join('..', 'data', 'not_trending_text_attributes_all.csv'))

# %%
img_df_not_trending.columns

# %%
img_df_2 = img_df_2.groupby('thumbnail_link').nth(0)
img_df_2_not_trending = img_df_2_not_trending.groupby('thumbnail_link').nth(0)

img_df_not_trending["image_filename"] = img_df_not_trending["image_filename"].apply(lambda x: x.replace("hqdefault.jpg", "default.jpg"))

# %%
img_df_not_trending.columns, img_df_2_not_trending.columns

# %% [markdown]
# #### Text DF preview

# %%
print(img_df.shape, img_df_2.shape, text_df.shape)
print(img_df_not_trending.shape, img_df_2_not_trending.drop_duplicates().shape, text_df_not_trending.shape)
text_df.head()

# %% [markdown]
# #### Visual DF preview

# %%
# need to convert 'list strings' into numpy arrays
for cname in img_df.columns:
    if 'histogram' in cname:
        img_df[cname] = img_df[cname].apply(lambda x : np.fromstring(x[1:-1], sep=' '))
        img_df_not_trending[cname] = img_df_not_trending[cname].apply(lambda x : np.fromstring(x[1:-1], sep=' '))

print(img_df.shape)
img_df.head()

# %% [markdown]
# ### Join dataframes with visual and text attributes

# %%
text_df["image_filename"] = text_df["thumbnail_link"].apply(lambda x : x.replace('/', '').replace(':', '_'))
text_df_not_trending["image_filename"] = text_df_not_trending["thumbnail_link"].apply(lambda x : x.replace('/', '').replace(':', '_'))

df = text_df.merge(img_df_2, on=["thumbnail_link"], how="left").merge(img_df, on=["image_filename"], how="left")
df_not_trending = text_df_not_trending.merge(img_df_2_not_trending, on=["thumbnail_link"], how="left").merge(img_df_not_trending, on=["image_filename"], how="left")
print(df.shape, df_not_trending.shape)
print(df.columns, df_not_trending.columns)

df["is_trending"] = True
df_not_trending["is_trending"] = False

df = pd.concat([df, df_not_trending])

df = df.reset_index()
df

# %%
list(df[['channel_title_embed', 'transormed_tags_embed', 'thumbnail_ocr_embed']].dtypes)


# %%
def cast_to_list(x):
    if x and x:
        return [float(num) for num in x[1:-1].replace("\n", "").split(" ") if num]
    else:
        return None

for column in ['channel_title_embed',  'thumbnail_ocr_embed', 'transormed_tags_embed', "title_embed"]:
    df[column] = df[column].apply(cast_to_list)

# %%
df[['channel_title_embed', 'transormed_tags_embed', 'thumbnail_ocr_embed', "title_embed"]].isnull().sum()

# %% [markdown]
# ## More textual features -> TF, TF IDF based

# %%
import csv

categories = {}
with open(os.path.join('..', 'data', 'categories.csv')) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=';')
    
    line_count = 0
    for row in csv_reader:
        if line_count == 0:
            line_count += 1
            continue
        else:
            categories[int(row[0])] = row[1]
        line_count += 1
        
    print(f'Processed {line_count} lines.')

# %%
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

vectorizer = TfidfVectorizer(stop_words='english')
vectors = vectorizer.fit_transform(np.unique(df["title"].values))

def top_tfidf_scores(corpus, n=15):
    # http://stackoverflow.com/questions/16078015/
    MAX_DF = 0.3 if len(corpus) > 10 else 1.0
    vectorizer = TfidfVectorizer(stop_words='english', max_features=100, max_df=MAX_DF, sublinear_tf=True)
    tfidf_result = vectorizer.fit_transform(corpus)
    
    scores = zip(vectorizer.get_feature_names(),
                 np.asarray(tfidf_result.sum(axis=0)).ravel())
    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)
    return sorted_scores[:n]

def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

titles_bow = []
N = 30

for i in df["category_id"].value_counts().keys():
    titles = np.unique(df[df["category_id"] == i]["title"].values)
    titles_bow.extend(w[0] for w in get_top_n_words(titles, n=N))
    titles_bow.extend(w[0] for w in top_tfidf_scores(titles, n=N))

titles_bow = list(sorted(set(titles_bow)))

def onehot_encode(x, BOW):
    x_lower = x.lower()
    result = np.zeros(shape=len(BOW), dtype=np.uint8)
    for idx, w in enumerate(BOW):
        if w in x_lower:
            result[idx] += 1
    return result

titles_onehot = []
df["title_onehot"] = df["title"].apply(lambda x : onehot_encode(x, titles_bow))

# %%
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

def reduce_histogram(series):
    series = list(filter(lambda x : not isinstance(x, float), series))
    if series:
        return tuple(np.mean(series, axis=0))
    else:
        return tuple(np.zeros(5)-1.0)

sns.set(rc={'figure.figsize':(18, 14)})

bow_agg_df = df.groupby("video_id").agg(
    title_onehot=("title_onehot", reduce_histogram),
    category_id=("category_id", lambda s : max(s)),
).reset_index()

X = np.stack(bow_agg_df["title_onehot"].values, axis=0)
pca = PCA(n_components=20)
X_pca = pca.fit_transform(X)

sns.scatterplot(
    x='c1', 
    y='c2',
      hue='category',
    size='has_category',
    data=pd.DataFrame({
      'c1': X_pca[:, 0],
      'c2': X_pca[:, 1],
      'category': list(map(lambda x : categories.get(int(x), "undefined"), bow_agg_df.fillna(0)["category_id"].values)),
        'has_category': list(map(lambda x : 1 if x == -1 else 15, bow_agg_df.fillna(-1)["category_id"].values))
  })); 


# %%
def onehot_encode(x, BOW):
    x_lower = x.lower()
    result = np.zeros(shape=len(BOW), dtype=np.uint8)
    for idx, w in enumerate(BOW):
        if w in x_lower:
            result[idx] += 1
    return result

def get_count_tfidf_embeddings(df, cname, split_cname, n_embeddings=20, N=50):
    vectorizer = TfidfVectorizer(stop_words='english')
    vectors = vectorizer.fit_transform(np.unique(df[cname].values))

    bow = []
    top_n_dict = {i : {} for i in df[split_cname].value_counts().keys()}
    
    for i in df[split_cname].value_counts().keys():
        words = np.unique(df[df[split_cname] == i][cname].values)
        words1 = [w[0] for w in get_top_n_words(words, n=N)]
        words2 = [w[0] for w in top_tfidf_scores(words, n=N)]
        bow.extend(words1); bow.extend(words2)
        top_n_dict[i]["top-n"] = words1
        top_n_dict[i]["tfidf"] = words2
        
    bow = list(sorted(set(bow)))    

    words_onehot = []
    new_cname = f"{cname}_onehot_{split_cname}"
    df[new_cname] = df[cname].apply(lambda x : onehot_encode(x, bow))

    bow_agg_df = df.groupby("video_id").agg(
        values=(new_cname, reduce_histogram),
        label=(split_cname, lambda s : max(s)),
    ).reset_index()

    X = np.stack(bow_agg_df["values"].values, axis=0)
    pca = PCA(n_components=n_embeddings)
    X_pca = pca.fit_transform(X)
    return X_pca , top_n_dict

def get_count_tfidf_embeddings_raw(df, cname, split_cname, N=30):
    vectorizer = TfidfVectorizer(stop_words='english')
    vectors = vectorizer.fit_transform(np.unique(df[cname].values))

    bow = []
    top_n_dict = {i : {} for i in df[split_cname].value_counts().keys()}
    
    for i in df[split_cname].value_counts().keys():
        words = np.unique(df[df[split_cname] == i][cname].values)
        words1 = [w[0] for w in get_top_n_words(words, n=N)]
        words2 = [w[0] for w in top_tfidf_scores(words, n=N)]
        bow.extend(words1); bow.extend(words2)
        top_n_dict[i]["top-n"] = words1
        top_n_dict[i]["tfidf"] = words2
        
    bow = list(sorted(set(bow)))    

    words_onehot = []
    new_cname = f"{cname}_onehot_{split_cname}"
    df[new_cname] = df[cname].apply(lambda x : onehot_encode(x, bow))

    bow_agg_df = df.groupby("video_id").agg(
        values=(new_cname, reduce_histogram),
#         label=(split_cname, lambda s : max(s)),
    ).reset_index()
    
    return bow_agg_df.values[:, 1], bow

# pca_result, top_n_dict = get_count_tfidf_embeddings(df, "title", "is_trending")
dfx, bow_list = get_count_tfidf_embeddings_raw(df, "title", "is_trending")
list(zip(dfx[0], bow_list))


# %% [markdown]
# ## Perform aggregations

# %%
def max_with_nans(series):
    result = max(series)
    if np.isnan(result):
        return -1.0
    else:
        return float(result) if isinstance(result, bool) else result
    
def reduce_medoid(series):
    series = np.array([row for row in series.to_numpy()])
    dist_matrix = scipy.spatial.distance_matrix(series, series)    
    return tuple(series[np.argmin(dist_matrix.sum(axis=0))])

agg_df = df.groupby("video_id").agg(
    trending_date=("trending_date", lambda s : len(set(s))), # how many days this video was trending,
    category_id=("category_id", lambda s : max(s)), # if a category was given, then take it :)
    publish_time=("publish_time", lambda s : max(s)), # we expect only one publish time anyway
    
    views_median=("views", "median"),
    views_max=("views", "max"),
    
    likes_median=("likes", "median"),
    likes_max=("likes", "max"),
    
    dislikes_median=("dislikes", "median"),
    dislikes_max=("dislikes", "max"),

    comments_disabled=("comments_disabled", "mean"),
    ratings_disabled=('ratings_disabled', "mean"),
    video_error_or_removed=('video_error_or_removed', "mean"),
    
    week_day=('week_day', "max"), # we don't expect different values here
    time_of_day=("time_of_day", "max"), # as they come from publish_time column
    month=('month', "max"),
    
    title_changes=("title", lambda s : len(set(s))), # how many different titles did we have?
    title_length_chars=('title_length_chars', "median"),
    title_length_tokens=("title_length_tokens", "median"),
    title_uppercase_ratio=("title_uppercase_ratio", "mean"),
    title_not_alnum_ratio=("title_not_alnum_ratio", "mean"),
    title_common_chars_count=("title_common_chars_count", "median"),
    
    channel_title_length_chars=("channel_title_length_chars", "median"),
    channel_title_length_tokens=("channel_title_length_tokens", "median"),
    
    tags_count=("tags_count", "median"),
    
    description_changes=("description", lambda s : len(set(s))), # how many changes of description?
    description_length_chars=("description_length_chars", "median"),
    description_length_tokens=("description_length_tokens", "median"),
    description_length_newlines=("description_length_newlines", "median"),
    description_uppercase_ratio=("description_uppercase_ratio", "mean"),
    description_url_count=("description_url_count", "median"),
    # description_top_domains_count=("description_top_domains_count", "median"),
    description_emojis_counts = ('emojis_counts', "median"),
    
    has_detection=("has_detection", max_with_nans),
    person_detected=("person_detected", max_with_nans),
    object_detected=("object_detected", max_with_nans),
    vehicle_detected=("vehicle_detected", max_with_nans),
    animal_detected=("animal_detected", max_with_nans),
    food_detected=("food_detected", max_with_nans),
    face_count=("face_count", max_with_nans),
    
    gray_histogram=("gray_histogram", reduce_histogram),
    hue_histogram=("hue_histogram", reduce_histogram),
    saturation_histogram=("saturation_histogram", reduce_histogram),
    value_histogram=("value_histogram", reduce_histogram),
    
    gray_median=("gray_median", "median"),
    hue_median=("hue_median", "median"),
    saturation_median=("saturation_median", "median"),
    value_median=("value_median", "median"),
    edges=("edges", "median"),
    
    ocr_length_tokens=('thumbnail_ocr_length', "median"),
    angry_count=('angry_count', "median"),
    surprise_count=('surprise_count', "median"),
    fear_count=('fear_count', "median"),
    happy_count=('happy_count', "median"),
    
    embed_title=('title_embed', reduce_medoid), 
    embed_channel_title=('channel_title_embed', reduce_medoid),
    embed_transormed_tags=('transormed_tags_embed', reduce_medoid), 
    embed_thumbnail_ocr=('thumbnail_ocr_embed', reduce_medoid),
    
    is_trending=('is_trending', lambda x: list(x)[0])
)

# OLD FEATURE:
agg_df["title_onehot"] = list(map(list, X_pca))


# IS TRENDING SPECIFIC + PCA
values, d1 = get_count_tfidf_embeddings(df, "title", "category_id")
agg_df["title_onehot_category_id_PCA"] = values.tolist()

values, d2 = get_count_tfidf_embeddings(df, "title", "is_trending")
agg_df["title_onehot_is_trending_PCA"] = values.tolist()

values, d3 = get_count_tfidf_embeddings(df, "description", "is_trending")
agg_df["description_onehot_is_trending_PCA"] = values.tolist()


# IS TRENDING SPECIFIC + NO PCA (raw ONE HOT)
bows = {}
values, bow = get_count_tfidf_embeddings_raw(df, "title", "category_id")
agg_df["title_onehot_category_id"] = values.tolist()
bows[("title", "category_id")] = bow

values, bow = get_count_tfidf_embeddings_raw(df, "title", "is_trending")
agg_df["title_onehot_is_trending"] = values.tolist()
bows[("title", "is_trending")] = bow

values, bow = get_count_tfidf_embeddings_raw(df, "description", "is_trending")
agg_df["description_onehot_is_trending"] = values.tolist()
bows[("description", "is_trending")] = bow

agg_df.head()


# %%
del df
del text_df
del img_df
del img_df_2

# %% [markdown]
# ### Extract subsets: numeric columns, non-numeric, histograms and videos with category_id given

# %%
agg_df_histograms = agg_df[[cname for cname in agg_df.columns if 'histogram' in cname]]
agg_df_numeric = agg_df[[cname for idx, cname in enumerate(agg_df.columns) if agg_df.dtypes[idx] in [np.int64, np.float64]]]
agg_df_not_numeric = agg_df[[cname for idx, cname in enumerate(agg_df.columns) if agg_df.dtypes[idx] not in [np.int64, np.float64]]]
agg_df_embeddings = agg_df[[cname for cname in agg_df.columns if cname.startswith('embed_')]]

agg_df_embeddings_numeric = pd.concat([
    pd.DataFrame(agg_df_embeddings[colname].values.tolist()).add_prefix(colname + '_')
    for colname in agg_df_embeddings.columns
], axis=1)


# %% [markdown]
# ### Analyze features stats
# > In order to do so, we normalize all the values into one range: 0-1, so that the variances are more comparable

# %%
# normalized_df = (agg_df_numeric - agg_df_numeric.mean()) / agg_df_numeric.std()

normalized_df = (agg_df_numeric - agg_df_numeric.min()) / (agg_df_numeric.max()-agg_df_numeric.min())

stats = normalized_df.describe()
stats

# %% [markdown]
# #### Which columns have the highest variance?

# %%
import matplotlib.pyplot as plt

std_deviations = stats.loc["std", :].sort_values(ascending=False)
std_plot = std_deviations.plot.bar(figsize=(14, 7), rot=80)
std_plot.get_figure().savefig("std_dev.pdf")
std_plot.plot()

# %%
std_deviations[ std_deviations < 0.1 ]

# %% [markdown]
# ## Feature corerlations
# > Using `-1.0` to denote missing values will potentially break the usefulness of correlation coef, so in the next heatmaps we split the features by their 'domain' (text or visual), skipping the missing values. This makes new coefficients more relevant.

# %%
import seaborn as sns

corr = agg_df_numeric[[
    cname for cname in agg_df_numeric.columns if cname not in ["trending_date", "category_id"]]
].corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)

xxx = ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)

# %%
ax.get_figure().savefig(os.path.join(FIGURES_DIR, "corr_all.pdf"))

# %% [markdown]
# ## Let's go deeper
# ### Visual attributes

# %%
visual_words = ['detect', 'face', 'gray', 'hue', 'saturation', 'value', 'edges', "ocr_length_tokens", "angry_count", "surprise_count", "fear_count", "happy_count"]
select_columns = [cname for cname in agg_df_numeric.columns if any([word in cname for word in visual_words])]

select_df = agg_df_numeric[select_columns]
select_df = select_df[select_df != -1.0]

corr = select_df.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True,
    annot=True
)

xxx = ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)

# %%
ax.get_figure().savefig(os.path.join(FIGURES_DIR, "corr_visual.pdf"))

# %% [markdown]
# ## Title, Channel Title + Description attributes

# %%
select_columns = [cname for cname in agg_df_numeric.columns if any([word in cname for word in ["title", "description"]])]

select_df = agg_df_numeric[select_columns]
select_df = select_df[select_df != -1.0]

corr = select_df.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True,
    annot=True
)

xxx = ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)

# %%
ax.get_figure().savefig(os.path.join(FIGURES_DIR, "corr_desc.pdf"))

# %% [markdown]
# ## Print most and least correlated feature for each column

# %%
import pandas as pd
import numpy as np

for idx, cname in enumerate(corr.index):
    if cname == 'category_id':
        continue
    
    max_corr = np.max(corr.loc[cname, corr.columns != cname])
    closest_idx = np.argmax(corr.loc[cname, (corr.columns != cname) & (corr.columns != "category_id")])
    print(cname, '-', corr.index[closest_idx], ' : ', max_corr)
    
    min_corr = np.min(corr.loc[cname, corr.columns != cname])
    furthest_idx = np.argmin(corr.loc[cname, (corr.columns != cname) & (corr.columns != "category_id")])
    print(cname, '-', corr.index[furthest_idx], ' : ', min_corr)
    
    print()

# %% [markdown]
# ### Embeddings comparison

# %%
for column_name in agg_df_embeddings.columns:
    print(column_name)
    agg_df_embeddings[column_name + "_argmax"] = agg_df_embeddings[column_name].copy().apply(np.argmax).copy()

# %%
agg_df_embeddings.columns


# %%
def cramers_corrected_stat(confusion_matrix):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher, 
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))


# %%
max_embed_column_names = [colname for colname in agg_df_embeddings.columns if colname.endswith("_argmax")]
corr = []
for column_name in max_embed_column_names:
    corr.append([])
    for column_name_2 in max_embed_column_names:
        confusion_matrix = pd.crosstab(agg_df_embeddings[column_name], agg_df_embeddings[column_name_2]).values
        # print(confusion_matrix)
        corr[-1].append(cramers_corrected_stat(confusion_matrix))

# %%
corr

# %%
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True,
    annot=True,
    xticklabels=max_embed_column_names,
    yticklabels=max_embed_column_names
)

xxx = ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)

yxx = ax.set_yticklabels(
    ax.get_yticklabels(),
    rotation=0,
)

# %%
ax.get_figure().savefig(os.path.join(FIGURES_DIR, "corr_embed.pdf"))


# %% [markdown]
# ### "Flatten" histogram values into columns

# %%
def transform_histogram_df(df):
    for cname in df.columns:
        if 'histogram' in cname:
            prefix = cname.split('_')[0]
            for i in range(5):
                df[f"{prefix}_{i}_bin"] = df[cname].apply(lambda x : x[i])
            df = df.drop(columns=[cname])
    return df

agg_df_histograms = transform_histogram_df(agg_df[[cname for cname in agg_df.columns if 'histogram' in cname]])
     
# VERY important, remove -1.0s! 
agg_df_histograms = agg_df_histograms[agg_df_histograms["gray_0_bin"] != -1.0]

# %%
corr = agg_df_histograms.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)

xxx = ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)

ax.hlines([0, 5, 10, 15, 20], *ax.get_xlim())
ax.vlines([0, 5, 10, 15, 20], *ax.get_xlim())

# %% [markdown]
# ### Histogram bins variances

# %%
normalized_df = (agg_df_histograms - agg_df_histograms.min()) / (agg_df_histograms.max()-agg_df_histograms.min())

stats = normalized_df.describe()

std_deviations = stats.loc["std", :].sort_values(ascending=False)
std_deviations.plot.bar(figsize=(14, 7), rot=45)

# %% [markdown]
# > Feature selection is performed using ANOVA F measure via the f_classif() function.

# %%
agg_df.columns

# %%

FEATURE_SELECTION_COLUMNS = [
    'category_id', 'publish_time', 'views_median',
    'views_max', 'likes_median', 'likes_max', 'dislikes_median',
    'dislikes_max', 'comments_disabled', 'ratings_disabled',
    'video_error_or_removed', 'week_day', 'time_of_day', 'month',
    'title_changes', 'title_length_chars', 'title_length_tokens',
    'title_uppercase_ratio', 'title_not_alnum_ratio',
    'title_common_chars_count', 'channel_title_length_chars',
    'channel_title_length_tokens', 'tags_count', 'description_changes',
    'description_length_chars', 'description_length_tokens',
    'description_length_newlines', 'description_uppercase_ratio',
    'description_url_count', 'description_emojis_counts', 'has_detection',
    'person_detected', 'object_detected', 'vehicle_detected',
    'animal_detected', 'food_detected', 'face_count', 'gray_histogram',
    'hue_histogram', 'saturation_histogram', 'value_histogram',
    'gray_median', 'hue_median', 'saturation_median', 'value_median',
    'edges', 'ocr_length_tokens', 'angry_count', 'surprise_count',
    'fear_count', 'happy_count', 'is_trending',

    # ONE HOT PCA MACARENA HERE
    # 'title_onehot', 
    'title_onehot_category_id_PCA',
    'title_onehot_is_trending_PCA', 'description_onehot_is_trending_PCA',
    'title_onehot_category_id', 'title_onehot_is_trending',
    'description_onehot_is_trending'
]

# %%
import math
from sklearn.model_selection import train_test_split
import json

#title_onehot_category_id
def transform_onehot_df(df):
    for cname in df.columns:
        if 'onehot' in cname and cname.endswith("PCA"):
            prefix = cname.replace("onehot", "")            
            for i in range(len(df[cname].values[0])):
                df[f"{prefix}_{i}_bin"] = df[cname].apply(lambda x : x[i])
            df = df.drop(columns=[cname])
    return df

def transform_onehot_df_with_vocabulary(df, source_cname, split_cname, bow_list):
    cname = f"{source_cname}_onehot_{split_cname}"
    for i in range(len(df[cname].values[0])):
        word_cname = cname + "_" + bow_list[i]
        df[word_cname] = df[cname].apply(lambda x : x[i])
    df = df.drop(columns=[cname])
    return df

df_feature_selection = agg_df[FEATURE_SELECTION_COLUMNS]

with open(os.path.join("..", "data", "API_categories.json"), "r") as handle:
    ids_to_categories_dict = json.load(handle)
    
df_feature_selection["category_id"] = df_feature_selection[["category_id"]].apply(
    # Fill missing categories
    lambda row : ids_to_categories_dict.get(row.name, -1) if math.isnan(row.category_id) or row.category_id == -1 else row.category_id,
    axis=1
)

df_feature_selection_numeric = transform_histogram_df(df_feature_selection)
df_feature_selection_numeric = transform_onehot_df(df_feature_selection)

for source_cname, split_cname in bows.keys():
    if split_cname == "category_id":
        # SKIP ONEHOTS FOR CATEGORY ID FOR NOW :)
        continue
    df_feature_selection_numeric = transform_onehot_df_with_vocabulary(
        df_feature_selection, 
        source_cname, 
        split_cname,
        bows[(source_cname, split_cname)]
    )
    
df_feature_selection_numeric = df_feature_selection_numeric[
    [cname for idx, cname in enumerate(df_feature_selection_numeric.columns) if df_feature_selection_numeric.dtypes[idx] in [np.int64, np.float64, np.bool]]
]

y = df_feature_selection_numeric["is_trending"].values
X = df_feature_selection_numeric.drop(columns=["is_trending", "category_id"]).fillna(-1.0)

X = (X - X.min()) / (X.max()-X.min()+1e-12) # normalize values - how about those that are missing?

# Splitting

train_idxs, test_idxs = train_test_split(np.arange(X.shape[0]), test_size=0.2)

X_columns = X.columns
X = X.values

X = X[train_idxs]
y = y[train_idxs]

X.shape

# %%
df_feature_selection_numeric[["is_trending", 'has_detection', 'person_detected', 'object_detected',
       'vehicle_detected', 'animal_detected', 'food_detected', 'gray_0_bin',
       'gray_1_bin', 'gray_2_bin', 'gray_3_bin', 'gray_4_bin'
                             ]]

# %%
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import json

selector = SelectKBest(score_func=f_classif, k=20)
fit = selector.fit(X, y)

# summarize scores
print(fit.scores_)
features = fit.transform(X)

cols = selector.get_support(indices=True)
print(X_columns[cols])

with open(os.path.join("..", "data", "anova_best_all_no_embeddings.json"), "w") as fp:
    json.dump(list(X_columns[cols]), fp)

# X_indices = np.arange(X.shape[-1])
# plt.bar(X_indices, -np.log10(selector.pvalues_), tick_label=X_columns)
# plt.bar(X_indices, selector.pvalues_, tick_label=X_columns)

# plt.xticks(rotation=45)


# %%
from sklearn.feature_selection import chi2

selector = SelectKBest(score_func=chi2, k=20)
fit = selector.fit(X, y)

# summarize scores
print(fit.scores_)
features = fit.transform(X)

cols = selector.get_support(indices=True)
print(X_columns[cols])

with open(os.path.join("..", "data", "chi2_best_all_no_embeddings.json"), "w") as fp:
    json.dump(list(X_columns[cols]), fp)

X_indices = np.arange(X.shape[-1])
# plt.bar(X_indices, selector.pvalues_)


# %%
from sklearn.feature_selection import mutual_info_classif

selector = SelectKBest(score_func=mutual_info_classif, k=20)
fit = selector.fit(X, y)

# summarize scores
print(fit.scores_)
features = fit.transform(X)

cols = selector.get_support(indices=True)
print(X_columns[cols])

with open(os.path.join("..", "data", "mi_best_all_no_embeddings.json"), "w") as fp:
    json.dump(list(X_columns[cols]), fp)

# %%
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.datasets import make_classification

# Create the RFE object and compute a cross-validated score.
svc = SVC(kernel="linear", class_weight='balanced')

# The "accuracy" scoring is proportional to the number of correct
# classifications
rfecv = RFECV(estimator=svc, step=5, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=15042020),
              scoring='accuracy', verbose=3)
rfecv.fit(X, y)

print("Optimal number of features : %d" % rfecv.n_features_)

# Plot number of features VS. cross-validation scores
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (nb of correct classifications)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

print(X_columns[rfecv.get_support(indices=True)])

# %%
df_feature_selection_numeric

# %%
print(df_feature_selection.shape)
agg_df_transformed = transform_histogram_df(df_feature_selection)
agg_df_transformed = transform_onehot_df(df_feature_selection)

print(agg_df_transformed.columns, agg_df_transformed.shape)


# %%
agg_df_transformed.iloc[train_idxs].to_csv(os.path.join("..", "data", "aggregated_train_no_embeddings.csv"))
agg_df_transformed.iloc[test_idxs].to_csv(os.path.join("..", "data", "aggregated_test_no_embeddings.csv"))

# %%
df_feature_selection[X_columns[rfecv.get_support(indices=True)]].to_csv(os.path.join("..", "data", "selected_features_all_no_embeddings.csv"))

# %% jupyter={"outputs_hidden": false}
selected = pd.read_csv(os.path.join(os.path.join("..", "data", "selected_features_all_no_embeddings.csv")))

# %% jupyter={"outputs_hidden": false}
with open(os.path.join("..", "data", "rfecv_best_all_no_embeddings.json"), "w") as fp:
    json.dump(list(selected.columns)[1:], fp)

# %%
